\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,float,amsfonts,enumitem,tcolorbox,hyperref}

\include{macros}

<<{r  global_options},echo=FALSE>>=
knitr::opts_chunk$set(include=TRUE,echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width=3, fig.height=3, fig.align='center',
                      fig.asp = 1
                      )
@

\title{STAT 154: Project 2 Cloud (Christopher Lau 26364374, Dat duc vu, 3031823757)}
\author{Release date: \textbf{Wednesday, April 10}}
\date{Due by: \textbf{11 PM, Wednesday, May 1}}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle

\section*{Please read carefully!}
\begin{itemize}
  \item It is a good idea to revisit your notes, slides and reading;
and synthesize their main points BEFORE doing the project.
  \item \emph{For this project, we adapt a zero tolerance policy with 
  incorrect/late submissions (no emails please) to Gradescope.}
  \item The recommended work of this project is at least 20 hours (at least 10 hours / person). Plan ahead and start early. 
  \item We need two things:
  \begin{enumerate}[label=(\alph*)]
    \item A main pdf report \textbf{(font size at least 11 pt, less or equal
    to 12 pages)} generated by Latex, Rnw or Word is required to be
    submitted to Gradescope.
    \begin{itemize}
      \item Provide top class (research-paper level) writing, useful
    well-labeled figures and no code in this pdf. Arrange text and figures
    compactly (.Rnw may not be very useful for this).
    \item You can choose a title for the report and a team name as per your
    liking (\emph{get creative!}). Do provide the names and student ID of
    your teammates below the title.
    \item Your report should conclude with an acknowledgment section, where
    you provide brief discussion about the contributions of each member,
    \textbf{and} the resources you used, credit all the help you took
    and briefly outline the way you proceeded with the project.
    \end{itemize}
    \item A link to your GitHub Repo at the end of your write-up that contains
    all your code (see Section 5 for more details).
  \end{enumerate}
  \item \textbf{Be visual \emph{and} quantitative:} Remember projects are graded differently when compared to homework---one line answer without explanation is usually not enough. Make your findings succinct and try to convince us with good arguments supported by numbers and figures.
Putting yourself in reader's shoes and reading the report out loud usually helps. The standards for grading are \emph{very high} this time. We will be very picky with figures: Lack of proper titles and axis labels will lead to loss of several points.
  
\end{itemize}

\newpage

\section*{Overview of the project} % (fold)
\label{sec:overview_of_the_project}

The goal of this project is the exploration and modeling of cloud detection in 
the polar regions based on radiance recorded automatically by the MISR sensor 
abroad the NASA satellite Terra. You will attempt to build a classification 
model to distinguish the presence of cloud from the absence of clouds in
the images using the available signals/features. Your dataset has ``expert
labels'' that can be used to train your models. When you evaluate your
results, imagine that your models will be used to distinguish
clouds from non-clouds on a large number of images that won't have these 
``expert'' labels.

On Piazza, you will find a zip archive with three files: \textbf{image1.txt},
\textbf{image2.txt}, \textbf{image3.txt}. Each contains one picture from
the satellite. Each of these files contains several rows each with 11 columns 
described in the Table~\ref{tab:feature} below. All five radiance angles
are raw features, while NDAI, SD, and CORR are features that are computed
based on subject matter knowledge. More information about the features is
in the article \textbf{yu2008.pdf}. The sensor data is multi-angle and recorded
in the red-band. 
For more information about MISR, see \textbf{http://www-misr.jpl.nasa.gov/}.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
 01 & y coordinate \\ \hline
 02 & x coordinate \\ \hline
 03 & expert label (+1 = cloud, -1 = not cloud, 0 unlabeled)\\ \hline
 04 & NDAI \\ \hline
 05 & SD \\ \hline
 06 & CORR \\ \hline
 07 & Radiance angle DF\\ \hline
 08 & Radiance angle CF\\ \hline
 09 & Radiance angle BF\\ \hline
 10 & Radiance angle AF\\ \hline
 11 & Radiance angle AN\\ \hline
\end{tabular}
\label{tab:feature}
\caption{Features in the cloud data.}
\end{table}

\section{Data Collection and Exploration (30 pts)}
\begin{enumerate}[label=(\alph*)]
\item \textbf{Write a half-page summary} of the paper, including at least
the purpose of the study, the data, the collection method, its conclusions
and potential impact.\\
\\
Global climate models are important to scientific and public interest, and a key component is the Earth’s increasing amount of atmospheric carbon dioxide. The clouds in the Arctic play an important role in modeling sensitivity to increasing air temperatures, thus leads to the attempt to measure the different properties of clouds. Unfortunately, the properties of clouds can be difficult to isolate, because ice/snow surfaces scatter light in a similar way, leading to cloud detection issues. NASA’s satellite launched the MISR, capable of measuring 360-km of Earth’s surface in four spectral bands at nine different angles. This leads to an impossibly large dataset that is impossible to classify except by some data analysis algorithm.\\ 
The goal is to build cloud detection algorithms that can process the MISR data set: searching for cloud-free conditions and using its correlations to find the reverse. The surface is modeled, and we find the (CORR) correlation of MISR images of the same scenery from different viewing directions, the standard deviation (SD) of MISR nadir camera pixel values across a scene, and a normalized difference angular index (NDAI) that characterizes the change in a scene with changes in the MISR view angle/direction. The data used in this study is collected from 10 MISR orbits of orbit path 26, which each contained six data units. The study concentrates on repeated visits so the experts could gain familiarity, improving the (EXPERT LABEL) process, which is the best method for producing validation data to compare against the model built by the algorithms and features.\\
We conclude that the three physical features - the linear correlation of MISR radiation measurements, standard deviation of MISR nadir red radiation measurements, and a normalized difference angular index, contain sufficient information to separate clouds from ice/snow surfaces. The ELCM algorithm based on the three features, which combines classification and clustering frameworks, is suitable for real-time MISR data processing and is more accurate than existing algorithms. Firstly, statisticians worked on the engineering of real-time processing, rather than simply performing ex post facto analysis on finished scientific work. Secondly, statistical thinking is important and has the ability to create solutions for modern scientific problems.
\item \textbf{Summarize} the data, i.e., $\%$ of pixels for the different
classes. \textbf{Plot well-labeled beautiful maps} using $x, y$ coordinates
the expert labels with color of the region based on the expert labels.\\
<<code,echo=FALSE>>=
library(tibble)
library(ggplot2)
library(dplyr)
library(tidyr)

df1 <- read.table('~/stats/154/project2/image_data/image1.txt')
df2 <- read.table('~/stats/154/project2/image_data/image2.txt')
df3 <- read.table('~/stats/154/project2/image_data/image3.txt')
columns <- c('y','x','expert','NDAI','SD','CORR','DF','CF','BF','AF','AN')
features <- c('expert','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(df1) <- columns
colnames(df2) <- columns
colnames(df3) <- columns
df <- rbind(df1,df2,df3)

perc1 <- df1 %>% group_by(expert) %>% summarise(round(n()/nrow(df1),2))
perc2 <- df2 %>% group_by(expert) %>% summarise(round(n()/nrow(df2),2))
perc3 <- df3 %>% group_by(expert) %>% summarise(round(n()/nrow(df3),2))
perc <- df %>% group_by(expert) %>% summarise(round(n()/nrow(df),2))
@
<<fig=TRUE,echo=FALSE>>=
ggplot(data=df1,aes(x=x,y=y)) + geom_point(aes(color=expert)) + ggtitle('Expert Label Plot of Image1')
@
\\
<<fig=TRUE,echo=FALSE>>=
ggplot(data=df2,aes(x=x,y=y)) + geom_point(aes(color=expert)) + ggtitle('Expert Label Plot of Image2')
@
\\
<<fig=TRUE,echo=FALSE>>=
ggplot(data=df3,aes(x=x,y=y)) + geom_point(aes(color=expert)) + ggtitle('Expert Label Plot of Image3')
@
\textbf{Do you observe some trend/pattern? Is an i.i.d. assumption for
the samples justified for this dataset?}\\
Cloud and cloud-free areas appear to have trends, or grouping. The appearance of each in a pixel, affect the possibility of cloud or cloud-free surrounding pixels, due to grouping or clustering. Therefore, the i.i.d. assumption is unjustifiable, as there are correlations and relationships within the data, based on the x, y coordinates. 
\item \textbf{Perform a visual and quantitative EDA} of the dataset, e.g.,
summarizing (i) pairwise relationship between the features themselves and
(ii) the relationship between the expert labels with the individual features.
<<echo=FALSE>>=
library(lattice)
corrDf <- function(data) {
  corr <- data %>% as.matrix %>% cor %>% as.data.frame %>% rownames_to_column(var = 'var1') %>% gather(var2, correlation, -var1)
  corr
}

filteredCorrDf <- function(corr) {
  corr1 <- corr %>% filter((correlation<1.0 & correlation>=0.6) | (correlation>-1.0 & correlation<=-.6))
  corr1$correlation <- round(corr1$correlation,4)
  corr1 <- corr1 %>% spread(var1,correlation) %>% arrange(order(features)) %>% column_to_rownames(var='var2')
  corr1[,features]
}

expertCorr <- function(corr) {
  corr %>% filter(var1 == 'expert')
}

corr1 <- filteredCorrDf(corrDf(df))
corr1
@
<<echo=FALSE,fig=TRUE>>=
levelplot(as.matrix(corr1),main='matrix of significant correlations among features',xlab='features',ylab='features')
@

<<echo=FALSE,fig=TRUE>>=
corr2 <- expertCorr(corrDf(df))
corr2
ggplot(corr2,aes(x=var2,y=correlation)) + geom_bar(stat='identity') + ggtitle('correlations of expert with features') + xlab('features')
@

<<echo=FALSE>>=
mean <- df %>% group_by(expert) %>% summarise_all(funs(mean))
mean
@
\textbf{Do you notice differences} between the two classes (cloud, no cloud)
based on the radiance or other features (CORR, NDAI, SD)?\\
The correlation graph tells me that cloud/cloud-free is positively related to the calculated features (NDAI, SD, CORR), and negatively to radiance, and there are differencesin the means of the features based on cloud/cloud-free labeling.
\end{enumerate}

\section{Preparation (40 pts)}
Now that we have done EDA with the data, we now prepare to train our model.
\begin{enumerate}[label=(\alph*)]
\item (Data Split) \textbf{Split the entire data} (image1.txt, image2.txt,
image3.txt) into three sets: training,  validation and test. Think carefully
about how to split the data. \textbf{Suggest at least two non-trivial different
ways} of splitting the data which takes into account that the data is not i.i.d.
<<echo=FALSE>>=
TRAINING <- 0.60
VALIDATION <- 0.20
TEST <- 0.20

trainset <- df[which(is.na(df$text)), ]
validset <- df[which(is.na(df$text)), ]
testset <- df[which(is.na(df$text)), ]

for (i in c(-1,0,1)) {
  data <- df %>% filter(expert==i)
  trainSize <- floor(TRAINING*nrow(data))
  validSize <- floor(VALIDATION*nrow(data))
  testSize <- floor(TEST*nrow(data))
  set.seed(7)
  indicesTraining <- sort(sample(seq_len(nrow(data)), size=trainSize,replace=FALSE))
  indicesNotTraining <- setdiff(seq_len(nrow(data)), indicesTraining)
  indicesValidation <- sort(sample(indicesNotTraining, size=validSize,replace=FALSE))
  indicesTest <- setdiff(indicesNotTraining, indicesValidation)

  dfTraining <- df[indicesTraining, ]
  trainset <- rbind(trainset,dfTraining)
  dfValidation <- df[indicesValidation, ]
  validset <- rbind(validset,dfValidation)
  dfTest <- df[indicesTest, ]
  testset <- rbind(testset,dfTest)
}
@

One way to split the data would be to use the expert label grouping, and ensure the approximate percentages of cloud, cloud-free, and ambiguous data from each split, is the same as the entire data set. You can do this by splitting every grouped value of expert label separetely. Another way to split the data is to directly use the expert label's percentages of cloud, cloud-free, and ambiguous data to sample into each split, but this is much less accurate than using the groups made by experts.
\item (Baseline) \textbf{Report the accuracy of a trivial classifier} which
sets all labels to -1 (cloud-free) on the validation set and on the test set. 
In what scenarios will such a classifier have high average accuracy?
\emph{Hint: Such a step provides a baseline to ensure that the classification
problems at hand is not trivial.}
<<echo=FALSE>>=
(nrow(validset %>% filter(expert==-1)) + nrow(testset %>% filter(expert==-1))) / (nrow(validset) + nrow(testset))
@

The accuracy of such a classifier would have high average accuracy on data sets with little to no clouds in the validation and test sets. A good example of this would be if the data was split poorly, and very little cloud covered data was split into the validation and test sets.
\item (First order importance) Assuming the expert labels as the
truth, and without using fancy classification methods, suggest
three of the ``best'' features, \textbf{using quantitative and visual justification}. Define your ``best'' feature criteria clearly. Only the relevant plots are necessary. Be sure to give this careful consideration, as it relates to subsequent problems.\\
Based on the graph plot I generated at the end of 1c, it tells me that the best correlations with the expert labeling are NDAI, and the 2 radiance angles AF, AN. Therefore, the three best features I would assume to be those three.
\item Write a generic cross validation (CV) function \textbf{CVgeneric} in R that takes a generic classifier, training features, training labels, number of folds $K$ and a loss function (at least classification accuracy should be there) as inputs and outputs the $K$-fold CV loss on the training set.  Please remember to put it in your github folder in Section 5.
\end{enumerate}

\section{Modeling (40 pts)}
We now try to fit different classification models and assess the fitted
models using different criterion. For the next three parts, we expect you
to try \emph{logistic regression and at least three other methods}.
\begin{enumerate}[label=(\alph*)]
\item \textbf{Try several classification methods and assess their fit using
cross-validation (CV). Provide a commentary on the assumptions for the
methods you tried and if they are satisfied in this case.} 
Since CV does not have a validation set, you can merge your training and
validation set to fit your CV model. 
\textbf{Report} the accuracies across
folds (and not just the average across folds) and the test accuracy. CV-results
for both the ways of creating folds (as answered in part 2(a)) should be
reported. Provide a brief commentary on the results. Make sure you honestly
mention all the classification methods you have  tried.

Method 1: Logic Regression Model

For Logistic Regression Model, the dependent variable should be in the range of 0 to 1 and that's why we rescaled the expert labels from -1,0,1 to 0,0.5,1 and we assume that it has the same interpretation. 

<<{r},echo=FALSE>>=
columns <- c('V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11')
colnames(trainset) <- columns
colnames(validset) <- columns
colnames(testset) <- columns
cloud = rbind(trainset, validset)
##testing data here
library(scales)
library(caret)
#Cutting into k-fold
cloud<-cloud[sample(nrow(cloud)),]
folds <- cut(seq(1,nrow(cloud)),breaks=10,labels=FALSE)
#Linear Regression Model
logistic_regression_results = c(1:10)
for(i in 1:10) {
  index <- which(folds==i,arr.ind=TRUE)
  train <- cloud[index, ]
  test <- cloud[-index, ]
  glm.fits = glm( rescale(V3, to = c(0,1)) ~ V1+V2+V4+V5+V6+V7+V8+V9+V10+V11,data=train,family=binomial )
  glm.probs=predict (glm.fits,subset(test,select=c(1,2,4,5,6,7,8,9,10,11)),type="response")
  glm.pred=rep(-1 , nrow(test))
  glm.pred[glm.probs > 1/3]=0
  glm.pred[glm.probs > 2/3]=1
  table(glm.pred, test$V3)
  error = mean(glm.pred != test$V3)
  logistic_regression_results[i] = 1 - error
}
glm.fits = glm( rescale(V3, to = c(0,1)) ~ V1+V2+V4+V5+V6+V7+V8+V9+V10+V11, data=cloud,family=binomial)
glm.probs= predict (glm.fits,subset(testset,select=c(1,2,4,5,6,7,8,9,10,11)),type="response")
glm.pred=rep(-1 , nrow(testset))
glm.pred[glm.probs > 1/3]=0
glm.pred[glm.probs > 2/3]=1
table(glm.pred, testset$V3)
error = mean(glm.pred != testset$V3)
test_accuracy = 1 - error
@
Logistic regression accuracies across 10 folds are:
<<{r},echo=FALSE>>=
logistic_regression_results
@
Average logistic regression accuracy is:
<<{r},echo=FALSE>>=
mean(logistic_regression_results)
@
Testing accuracy is:
<<{r},echo=FALSE>>=
test_accuracy
@

Method 2: Linear Discriminant Analysis

<<{r},echo=FALSE>>=
library(MASS)
lda_results = c(1:10)
#Cutting into k-fold
cloud<-cloud[sample(nrow(cloud)),]
folds <- cut(seq(1,nrow(cloud)),breaks=10,labels=FALSE)
for(i in 1:10) {
  index <- which(folds==i,arr.ind=TRUE)
  train <- cloud[index, ]
  test <- cloud[-index, ]
  lda.fit = lda( V3 ~ V1+V2+V4+V5+V6+V7+V8+V9+V10+V11,data=train)
  lda.pred = predict(lda.fit, subset(test,select=c(1,2,4,5,6,7,8,9,10,11)))
  lda.class = lda.pred$class
  table(lda.class, test$V3)
  error = mean(lda.class != test$V3)
  lda_results[i] = 1 - error
}
lda.fit = lda( V3 ~ V1+V2+V4+V5+V6+V7+V8+V9+V10+V11,data=cloud)
lda.pred = predict(lda.fit, subset(testset,select=c(1,2,4,5,6,7,8,9,10,11)))
lda.class = lda.pred$class
table(lda.class, testset$V3)
error = mean(lda.class != testset$V3)
test_accuracy = 1 - error
@
LDA accuracies across 10 folds are:
<<{r},echo=FALSE>>=
lda_results
@
Average LDA regression accuracy is:
<<{r},echo=FALSE>>=
mean(lda_results)
@
Testing accuracy is:
<<{r},echo=FALSE>>=
test_accuracy
@

Method 3: QDA
<<{r},echo=FALSE>>=
library(MASS)
qda_results = c(1:10)
#Cutting into k-fold
cloud<-cloud[sample(nrow(cloud)),]
folds <- cut(seq(1,nrow(cloud)),breaks=10,labels=FALSE)
for(i in 1:10) {
  index <- which(folds==i,arr.ind=TRUE)
  train <- cloud[index, ]
  test <- cloud[-index, ]
  qda.fit = qda( V3 ~ V1+V2+V4+V5+V6+V7+V8+V9+V10+V11,data=train)
  qda.pred = predict(qda.fit, subset(test,select=c(1,2,4,5,6,7,8,9,10,11)))
  qda.class = qda.pred$class
  table(qda.class, test$V3)
  error = mean(qda.class != test$V3)
  qda_results[i] = 1 - error
}
qda.fit = qda( V3 ~ V1+V2+V4+V5+V6+V7+V8+V9+V10+V11,data=cloud)
qda.pred = predict(qda.fit, subset(testset,select=c(1,2,4,5,6,7,8,9,10,11)))
qda.class = qda.pred$class
error = mean(qda.class != testset$V3)
test_accuracy = 1 - error
@
QDA accuracies across 10 folds are:
<<{r},echo=FALSE>>=
qda_results
@
Average QDA regression accuracy is:
<<{r},echo=FALSE>>=
mean(qda_results)
@
Testing accuracy is:
<<{r},echo=FALSE>>=
test_accuracy
@

\item \textbf{Use ROC curves to compare the different methods.} 
Choose a cutoff value and highlight it on the ROC curve. Explain your choice
of the cutoff value. 

ROC Curve for Logistic Regression:

<<{r},echo=FALSE,fig=TRUE>>=
library(pROC)
pROC_obj <- roc(testset$V3, glm.pred,
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
abline(v=0.5)
@
ROC Curve for LDA:

<<{r},echo=FALSE,fig=TRUE>>=
library(pROC)
pROC_obj <- roc(testset$V3, as.numeric(lda.class) - 2,
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
abline(v=0.5)
@
ROC Curve for QDA:

<<{r},echo=FALSE,fig=TRUE>>=
library(pROC)
pROC_obj <- roc(testset$V3, as.numeric(qda.class) - 2,
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)
abline(v=0.5)
@
\item (Bonus) Assess the fit using other relevant metrics.
\end{enumerate}

\section{Diagnostics (50 pts)}
\emph{Disclaimer:} The questions in this section are open-ended.
Be visual and quantitative! The gold standard arguments would be able to
convince National Aeronautics and  Space Administration (NASA) to use your
classification method---in which case Bonus points will be awarded.
\begin{enumerate}[label=(\alph*)]
\item Do an in-depth analysis of a good classification model
of your choice by showing some diagnostic plots or information related to
convergence or parameter estimation.
\item For your best classification model(s), do you notice any patterns in the 
misclassification errors? Again, use quantitative and visual methods of analysis. Do you notice problems in particular regions, or in
specific ranges of feature values?
\item Based on parts 4(a) and 4(b), can you think of a better classifier?
How well do you think your model will work on future data without expert 
labels?
\item Do your results in parts 4(a) and 4(b) change as you modify the
way of splitting the data?
\item Write a paragraph for your conclusion.
\end{enumerate}

\section{Reproducibility (10 pts)}
In addition to a writeup of the above results, please provide a one-line link 
to a public GitHub repository containing everything necessary to reproduce
your writeup. Specifically, imagine that at some point an error is discovered
in the three image files, and a future researcher wants to check whether
your results hold up with the new, corrected image files. This researcher
should be able to easily re-run all your code and produce all your figures
and tables. This repository should contain:
\begin{enumerate}[label=(\roman*)]
  \item The pdf of the report,
  \item the raw Latex, Rnw or Word used to generate your report,
  \item your R code (with CVgeneric function in a separate R file),
  \item a README file describing, in detail, how to reproduce your paper
  from scratch (assume researcher has access to the images).
\end{enumerate}
https://github.com/chrislaustopher/stat154proj2 \\
You might want to take a look at the GitHub's tutorials 
\href{https://guides.github.com/}{https://guides.github.com/}.

\section*{Final remarks} % (fold)
\label{sec:final_remarks}
% As a reminder:
\begin{itemize}
  \item Make sure to read the instructions for the submission on Page
  1.
  \item Note that we will enforce a \textbf{zero tolerance policy for last
  minute / late requests (no emails please) this time.} Start early and
  plan ahead. If something
  is falling apart or not working, see us in office hours.
\end{itemize}

\end{document}